{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82216917",
   "metadata": {},
   "source": [
    "# Data Analysis - All dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415366ab",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6d1f07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anape\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anape\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anape\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anape\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter,OrderedDict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739945a",
   "metadata": {},
   "source": [
    "Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8286ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858832\n"
     ]
    }
   ],
   "source": [
    "df =  pd.read_csv('tweet_all.csv',  sep=';', on_bad_lines='skip',lineterminator='\\r',low_memory=False)\n",
    "print(len(df))\n",
    "text = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfabad2",
   "metadata": {},
   "source": [
    "Cleaning 1 - Removing rows with specific words (the whole row, because is related to a subject that won't be useful) - here it also eliminates if the word has is a hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24a227f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame length: 767078\n"
     ]
    }
   ],
   "source": [
    "# Specify the list of specific words you want to filter out\n",
    "specific_words = ['trânsito', 'rodovia', 'tráfego', 'ecovias','TIM', 'tim', 'Kaysar' , 'novela', 'Bandeirantes', 'operacaobetalab', 'bbb', 'transito', 'trânsito', 'orcars', 'oscar', 'grammy', 'grammys', 'kaysar', 'timbetalab', ' masterchefbr', 'enem', 'futebol', ' bundesliga', 'corinthians']\n",
    "\n",
    "# Fill NaN values in the 'text' column with an empty string\n",
    "df['text'].fillna('', inplace=True)\n",
    "\n",
    "# Create a boolean mask for rows containing specific words in the 'text' column\n",
    "mask = df['text'].str.contains('|'.join(specific_words), case=False)\n",
    "\n",
    "# Apply the mask to filter out rows with specific words\n",
    "df_filtered = df[~mask]\n",
    "\n",
    "print(\"Filtered DataFrame length:\", len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52cfdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_filtered.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b064b",
   "metadata": {},
   "source": [
    "Cleaning 2 - removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1b7084dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text  \\\n",
      "0   561675512373067776  RT @cartacapital: A União Europeia tem virado ...   \n",
      "1   561676165795287040  RT @cartacapital: A União Europeia tem virado ...   \n",
      "2   561677292938592256                         Va estar refugiado seguro.   \n",
      "3   561677511655190528  RT @cartacapital: A União Europeia tem virado ...   \n",
      "4   561678375488872448                                          Refugiado   \n",
      "5   561678577985654784  RT @cartacapital: A União Europeia tem virado ...   \n",
      "6   561679087073107968    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7   561679088255901696    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   561680173494706177  RT @JornalOGlobo: 'A escalada do sofrimento do...   \n",
      "9   561682696871882752                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "10  561682887834370048  África Central: fotos retratam dramas médicos ...   \n",
      "11  561683636249190400  RT @cartacapital: A União Europeia tem virado ...   \n",
      "12  561684241390796800         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "14  561686422915391488      Gostei de um vídeo @YouTube  OF - IMIGRAÇÃO   \n",
      "16  561692929597505538  Fotos retratam dramas médicos de refugiados na...   \n",
      "17  561693380951162882  RT @cartacapital: A União Europeia tem virado ...   \n",
      "18  561695226377416704  RT @g1: Fotos retratam dramas médicos de refug...   \n",
      "23  561704646025019392      Gostei de um vídeo @YouTube  OF - IMIGRAÇÃO   \n",
      "24  561706494903996417   Motociclista escapa de um assalto na imigrantes    \n",
      "25  561707327456559104  Parece uma pintura... “@cartacapital: A UE tem...   \n",
      "26  561707657187573761         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "27  561708309821276162  @juhbstew n falei de ana nenhuma falei de algu...   \n",
      "28  561708959326019584  @juhbstew e eu la quero saber? acabei de falar...   \n",
      "29  561709319700631552  RT @cartacapital: A União Europeia tem virado ...   \n",
      "30  561711337412526081  @_tininhaaa simmm, no jornal nacional! matéria...   \n",
      "31  561713569080041472  Adicionei um vídeo a uma playlist @YouTube  Im...   \n",
      "32  561713583529422848            Gostei de um vídeo @YouTube  Imigrantes   \n",
      "33  561721631156871168  ARTIGO: A escalada do sofrimento dos refugiado...   \n",
      "34  561722824700620800  isso já gerou situacoes engraçadas — qnd eu fi...   \n",
      "35  561723675188404225                 Aguante River♥ Manga de refugiados   \n",
      "36  561723709535571968  @paulllynda @scalon_ eu não entendo a magia qu...   \n",
      "37  561724641312391169  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "38  561724703665303553  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "39  561724774603165696  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "40  561724922150785026  @Ivan_Marcondes ,se tivesse lá ia gritar:\"Olha...   \n",
      "41  561725170722029570  @Rene_Silva_RJ @samegui a #cpbr8 ter voltado p...   \n",
      "42  561725336904544256  RT @Estadao: UE oferece US$ 42 milhões ao Líba...   \n",
      "43  561725794792521731  RT @AC_Delgado: \"@cartacapital: A União Europe...   \n",
      "44  561726402014498816   Quando for grande quero ser imigrante em Coimbra   \n",
      "45  561736382058889216              I liked a @YouTube video  IMIGRAÇÃO   \n",
      "46  561745182786191360  #Fotos retratam dramas médicos de refugiados n...   \n",
      "47  561751591418888192  Americano com sobrenome Diaz, alguém chama a i...   \n",
      "48  561759611519508480  Se o serviço de imigração aparece aí agora, ge...   \n",
      "49  561759873227309056  RT @WalkenesLagares: Se o serviço de imigração...   \n",
      "50  561760145093689344  RT @WalkenesLagares: Se o serviço de imigração...   \n",
      "51  561760264740433920  RT @WalkenesLagares: Se o serviço de imigração...   \n",
      "52  561760708095119360  RT @WalkenesLagares: Se o serviço de imigração...   \n",
      "53  561761911122165760  RT @WalkenesLagares: Se o serviço de imigração...   \n",
      "54  561762164529438724  Angelina Jolie desabafa após visitar campo de ...   \n",
      "55  561777590894333952  Quem nunca se apaixonou pelo policial da imigr...   \n",
      "\n",
      "             created_at   author_id lang in_reply_to_user_id  \n",
      "0   2015-02-01 00:00:52    45630114   pt                None  \n",
      "1   2015-02-01 00:03:28  2790573696   pt                None  \n",
      "2   2015-02-01 00:07:56  2169009544   es                None  \n",
      "3   2015-02-01 00:08:48  2649502406   pt                None  \n",
      "4   2015-02-01 00:12:14  2769334138   es                None  \n",
      "5   2015-02-01 00:13:03   498201900   pt                None  \n",
      "6   2015-02-01 00:15:04    20916633   pt                None  \n",
      "7   2015-02-01 00:15:04    34986682   pt                None  \n",
      "8   2015-02-01 00:19:23    50374800   pt                None  \n",
      "9   2015-02-01 00:29:25   614468732   pt                None  \n",
      "10  2015-02-01 00:30:10   321070043   pt                None  \n",
      "11  2015-02-01 00:33:09    79073968   pt                None  \n",
      "12  2015-02-01 00:35:33   192037162   pt                None  \n",
      "14  2015-02-01 00:44:13   341329840   pt                None  \n",
      "16  2015-02-01 01:10:04   356916034   pt                None  \n",
      "17  2015-02-01 01:11:52  2800332826   pt                None  \n",
      "18  2015-02-01 01:19:12   744230935   pt                None  \n",
      "23  2015-02-01 01:56:38   930330596   pt                None  \n",
      "24  2015-02-01 02:03:59  1235082847   pt                None  \n",
      "25  2015-02-01 02:07:17    38970838   pt            16632084  \n",
      "26  2015-02-01 02:08:36   122163624   pt                None  \n",
      "27  2015-02-01 02:11:11   100101991   pt            50062276  \n",
      "28  2015-02-01 02:13:46   100101991   pt            50062276  \n",
      "29  2015-02-01 02:15:12  2595454507   pt                None  \n",
      "30  2015-02-01 02:23:13   109101279   pt           312068863  \n",
      "31  2015-02-01 02:32:05    47383723   pt                None  \n",
      "32  2015-02-01 02:32:09    47383723   pt                None  \n",
      "33  2015-02-01 03:04:07   527945055   pt                None  \n",
      "34  2015-02-01 03:08:52    11436952   pt                None  \n",
      "35  2015-02-01 03:12:15  1486067329   es                None  \n",
      "36  2015-02-01 03:12:23   438568354   pt           104002307  \n",
      "37  2015-02-01 03:16:05   887707008   pt                None  \n",
      "38  2015-02-01 03:16:20   566581953   pt                None  \n",
      "39  2015-02-01 03:16:37   887707008   pt                None  \n",
      "40  2015-02-01 03:17:12   533292091   pt            19964288  \n",
      "41  2015-02-01 03:18:11    17675080   pt            55118656  \n",
      "42  2015-02-01 03:18:51  1134966854   pt                None  \n",
      "43  2015-02-01 03:20:40   377749571   pt                None  \n",
      "44  2015-02-01 03:23:05  2714791298   pt                None  \n",
      "45  2015-02-01 04:02:44   197265897   pt                None  \n",
      "46  2015-02-01 04:37:42   400701289   pt                None  \n",
      "47  2015-02-01 05:03:10   336629154   pt                None  \n",
      "48  2015-02-01 05:35:03   113765032   pt                None  \n",
      "49  2015-02-01 05:36:05   393144290   pt                None  \n",
      "50  2015-02-01 05:37:10   174905247   pt                None  \n",
      "51  2015-02-01 05:37:38   253316454   pt                None  \n",
      "52  2015-02-01 05:39:24  1126498818   pt                None  \n",
      "53  2015-02-01 05:44:11    59321805   pt                None  \n",
      "54  2015-02-01 05:45:11   173944935   pt                None  \n",
      "55  2015-02-01 06:46:29    33411162   pt                None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\62204965.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(url_pattern, '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match URLs\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# Remove URLs from the 'text' column using regular expressions\n",
    "df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(url_pattern, '', regex=True)\n",
    "\n",
    "print(df_filtered.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b8ef1",
   "metadata": {},
   "source": [
    "Cleaning 3 - removing specific words from the column 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d2d20d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text  \\\n",
      "0   561675512373067776   @cartacapital: A União Europeia tem virado as...   \n",
      "1   561676165795287040   @cartacapital: A União Europeia tem virado as...   \n",
      "2   561677292938592256                         Va estar refugiado seguro.   \n",
      "3   561677511655190528   @cartacapital: A União Europeia tem virado as...   \n",
      "4   561678375488872448                                          Refugiado   \n",
      "5   561678577985654784   @cartacapital: A União Europeia tem virado as...   \n",
      "6   561679087073107968    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7   561679088255901696    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   561680173494706177   @JornalOGlobo: 'A escalada do sofrimento dos ...   \n",
      "9   561682696871882752                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "10  561682887834370048  África Central: fotos retratam dramas médicos ...   \n",
      "11  561683636249190400   @cartacapital: A União Europeia tem virado as...   \n",
      "12  561684241390796800         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "14  561686422915391488      Gostei de um vídeo @YouTube  OF - IMIGRAÇÃO   \n",
      "16  561692929597505538  Fotos retratam dramas médicos de refugiados na...   \n",
      "17  561693380951162882   @cartacapital: A União Europeia tem virado as...   \n",
      "18  561695226377416704   @g1: Fotos retratam dramas médicos de refugia...   \n",
      "23  561704646025019392      Gostei de um vídeo @YouTube  OF - IMIGRAÇÃO   \n",
      "24  561706494903996417   Motociclista escapa de um assalto na imigrantes    \n",
      "25  561707327456559104  Parece uma pintura... “@cartacapital: A UE tem...   \n",
      "26  561707657187573761         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "27  561708309821276162  @juhbstew n falei de ana nenhuma falei de algu...   \n",
      "28  561708959326019584  @juhbstew e eu la quero saber? acabei de falar...   \n",
      "29  561709319700631552   @cartacapital: A União Europeia tem virado as...   \n",
      "30  561711337412526081  @_tininhaaa simmm, no jornal nacional! matéria...   \n",
      "31  561713569080041472  Adicionei um vídeo a uma playlist @YouTube  Im...   \n",
      "32  561713583529422848            Gostei de um vídeo @YouTube  Imigrantes   \n",
      "33  561721631156871168  ARTIGO: A escalada do sofrimento dos refugiado...   \n",
      "34  561722824700620800  isso já gerou situacoes engraçadas — qnd eu fi...   \n",
      "35  561723675188404225                 Aguante River♥ Manga de refugiados   \n",
      "36  561723709535571968  @paulllynda @scalon_ eu não entendo a magia qu...   \n",
      "37  561724641312391169  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "38  561724703665303553  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "39  561724774603165696  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "40  561724922150785026  @Ivan_Marcondes ,se tivesse lá ia gritar:\"Olha...   \n",
      "41  561725170722029570  @Rene_Silva_RJ @samegui a #cpbr8 ter voltado p...   \n",
      "42  561725336904544256   @Estadao: UE oferece US$ 42 milhões ao Líbano...   \n",
      "43  561725794792521731   @AC_Delgado: \"@cartacapital: A União Europeia...   \n",
      "44  561726402014498816   Quando for grande quero ser imigrante em Coimbra   \n",
      "45  561736382058889216              I liked a @YouTube video  IMIGRAÇÃO   \n",
      "46  561745182786191360  #Fotos retratam dramas médicos de refugiados n...   \n",
      "47  561751591418888192  Americano com sobrenome Diaz, alguém chama a i...   \n",
      "48  561759611519508480  Se o serviço de imigração aparece aí agora, ge...   \n",
      "49  561759873227309056   @WalkenesLagares: Se o serviço de imigração a...   \n",
      "50  561760145093689344   @WalkenesLagares: Se o serviço de imigração a...   \n",
      "51  561760264740433920   @WalkenesLagares: Se o serviço de imigração a...   \n",
      "52  561760708095119360   @WalkenesLagares: Se o serviço de imigração a...   \n",
      "53  561761911122165760   @WalkenesLagares: Se o serviço de imigração a...   \n",
      "54  561762164529438724  Angelina Jolie desabafa após visitar campo de ...   \n",
      "55  561777590894333952  Quem nunca se apaixonou pelo policial da imigr...   \n",
      "\n",
      "             created_at   author_id lang in_reply_to_user_id  \n",
      "0   2015-02-01 00:00:52    45630114   pt                None  \n",
      "1   2015-02-01 00:03:28  2790573696   pt                None  \n",
      "2   2015-02-01 00:07:56  2169009544   es                None  \n",
      "3   2015-02-01 00:08:48  2649502406   pt                None  \n",
      "4   2015-02-01 00:12:14  2769334138   es                None  \n",
      "5   2015-02-01 00:13:03   498201900   pt                None  \n",
      "6   2015-02-01 00:15:04    20916633   pt                None  \n",
      "7   2015-02-01 00:15:04    34986682   pt                None  \n",
      "8   2015-02-01 00:19:23    50374800   pt                None  \n",
      "9   2015-02-01 00:29:25   614468732   pt                None  \n",
      "10  2015-02-01 00:30:10   321070043   pt                None  \n",
      "11  2015-02-01 00:33:09    79073968   pt                None  \n",
      "12  2015-02-01 00:35:33   192037162   pt                None  \n",
      "14  2015-02-01 00:44:13   341329840   pt                None  \n",
      "16  2015-02-01 01:10:04   356916034   pt                None  \n",
      "17  2015-02-01 01:11:52  2800332826   pt                None  \n",
      "18  2015-02-01 01:19:12   744230935   pt                None  \n",
      "23  2015-02-01 01:56:38   930330596   pt                None  \n",
      "24  2015-02-01 02:03:59  1235082847   pt                None  \n",
      "25  2015-02-01 02:07:17    38970838   pt            16632084  \n",
      "26  2015-02-01 02:08:36   122163624   pt                None  \n",
      "27  2015-02-01 02:11:11   100101991   pt            50062276  \n",
      "28  2015-02-01 02:13:46   100101991   pt            50062276  \n",
      "29  2015-02-01 02:15:12  2595454507   pt                None  \n",
      "30  2015-02-01 02:23:13   109101279   pt           312068863  \n",
      "31  2015-02-01 02:32:05    47383723   pt                None  \n",
      "32  2015-02-01 02:32:09    47383723   pt                None  \n",
      "33  2015-02-01 03:04:07   527945055   pt                None  \n",
      "34  2015-02-01 03:08:52    11436952   pt                None  \n",
      "35  2015-02-01 03:12:15  1486067329   es                None  \n",
      "36  2015-02-01 03:12:23   438568354   pt           104002307  \n",
      "37  2015-02-01 03:16:05   887707008   pt                None  \n",
      "38  2015-02-01 03:16:20   566581953   pt                None  \n",
      "39  2015-02-01 03:16:37   887707008   pt                None  \n",
      "40  2015-02-01 03:17:12   533292091   pt            19964288  \n",
      "41  2015-02-01 03:18:11    17675080   pt            55118656  \n",
      "42  2015-02-01 03:18:51  1134966854   pt                None  \n",
      "43  2015-02-01 03:20:40   377749571   pt                None  \n",
      "44  2015-02-01 03:23:05  2714791298   pt                None  \n",
      "45  2015-02-01 04:02:44   197265897   pt                None  \n",
      "46  2015-02-01 04:37:42   400701289   pt                None  \n",
      "47  2015-02-01 05:03:10   336629154   pt                None  \n",
      "48  2015-02-01 05:35:03   113765032   pt                None  \n",
      "49  2015-02-01 05:36:05   393144290   pt                None  \n",
      "50  2015-02-01 05:37:10   174905247   pt                None  \n",
      "51  2015-02-01 05:37:38   253316454   pt                None  \n",
      "52  2015-02-01 05:39:24  1126498818   pt                None  \n",
      "53  2015-02-01 05:44:11    59321805   pt                None  \n",
      "54  2015-02-01 05:45:11   173944935   pt                None  \n",
      "55  2015-02-01 06:46:29    33411162   pt                None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\2467382082.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(pattern, '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Define a list of specific words you want to remove\n",
    "specific_words_to_remove = ['RT', 'q' , 'vc', 'htt...', 'http...']\n",
    "\n",
    "# Create a regular expression pattern to match the specific words\n",
    "pattern = r'\\b(?:' + '|'.join(specific_words_to_remove) + r')\\b'\n",
    "\n",
    "# Remove the specific words from the 'text' column using regular expressions\n",
    "df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "print(df_filtered.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d71078",
   "metadata": {},
   "source": [
    "Cleaning 4 - removing usernames and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1af81472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text  \\\n",
      "0   561675512373067776   : A União Europeia tem virado as costas aos r...   \n",
      "1   561676165795287040   : A União Europeia tem virado as costas aos r...   \n",
      "2   561677292938592256                         Va estar refugiado seguro.   \n",
      "3   561677511655190528   : A União Europeia tem virado as costas aos r...   \n",
      "4   561678375488872448                                          Refugiado   \n",
      "5   561678577985654784   : A União Europeia tem virado as costas aos r...   \n",
      "6   561679087073107968    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7   561679088255901696    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   561680173494706177   : 'A escalada do sofrimento dos refugiados', ...   \n",
      "9   561682696871882752                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "10  561682887834370048  África Central: fotos retratam dramas médicos ...   \n",
      "11  561683636249190400   : A União Europeia tem virado as costas aos r...   \n",
      "12  561684241390796800         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "14  561686422915391488              Gostei de um vídeo   OF - IMIGRAÇÃO   \n",
      "16  561692929597505538  Fotos retratam dramas médicos de refugiados na...   \n",
      "17  561693380951162882   : A União Europeia tem virado as costas aos r...   \n",
      "18  561695226377416704   : Fotos retratam dramas médicos de refugiados...   \n",
      "23  561704646025019392              Gostei de um vídeo   OF - IMIGRAÇÃO   \n",
      "24  561706494903996417   Motociclista escapa de um assalto na imigrantes    \n",
      "25  561707327456559104  Parece uma pintura... “: A UE tem virado as co...   \n",
      "26  561707657187573761         I'm at Esfiha Imigrantes in São Paulo, SP    \n",
      "27  561708309821276162   n falei de ana nenhuma falei de alguem que ch...   \n",
      "28  561708959326019584   e eu la quero saber? acabei de falar que foi ...   \n",
      "29  561709319700631552   : A União Europeia tem virado as costas aos r...   \n",
      "30  561711337412526081   simmm, no jornal nacional! matéria sobre o ta...   \n",
      "31  561713569080041472     Adicionei um vídeo a uma playlist   Imigrantes   \n",
      "32  561713583529422848                    Gostei de um vídeo   Imigrantes   \n",
      "33  561721631156871168  ARTIGO: A escalada do sofrimento dos refugiado...   \n",
      "34  561722824700620800  isso já gerou situacoes engraçadas — qnd eu fi...   \n",
      "35  561723675188404225                 Aguante River♥ Manga de refugiados   \n",
      "36  561723709535571968    eu não entendo a magia que é um monte de imi...   \n",
      "37  561724641312391169  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "38  561724703665303553  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "39  561724774603165696  Por Brazilian Defence League\\n\\nRefugiado do g...   \n",
      "40  561724922150785026   ,se tivesse lá ia gritar:\"Olha a imigração!\"#...   \n",
      "41  561725170722029570    a #cpbr8 ter voltado para o Imigrantes ficou...   \n",
      "42  561725336904544256   : UE oferece US$ 42 milhões ao Líbano para at...   \n",
      "43  561725794792521731   : \": A União Europeia tem virado as costas ao...   \n",
      "44  561726402014498816   Quando for grande quero ser imigrante em Coimbra   \n",
      "45  561736382058889216                      I liked a  video  IMIGRAÇÃO   \n",
      "46  561745182786191360  #Fotos retratam dramas médicos de refugiados n...   \n",
      "47  561751591418888192  Americano com sobrenome Diaz, alguém chama a i...   \n",
      "48  561759611519508480  Se o serviço de imigração aparece aí agora, ge...   \n",
      "49  561759873227309056   : Se o serviço de imigração aparece aí agora,...   \n",
      "50  561760145093689344   : Se o serviço de imigração aparece aí agora,...   \n",
      "51  561760264740433920   : Se o serviço de imigração aparece aí agora,...   \n",
      "52  561760708095119360   : Se o serviço de imigração aparece aí agora,...   \n",
      "53  561761911122165760   : Se o serviço de imigração aparece aí agora,...   \n",
      "54  561762164529438724  Angelina Jolie desabafa após visitar campo de ...   \n",
      "55  561777590894333952  Quem nunca se apaixonou pelo policial da imigr...   \n",
      "\n",
      "             created_at   author_id lang in_reply_to_user_id  \n",
      "0   2015-02-01 00:00:52    45630114   pt                None  \n",
      "1   2015-02-01 00:03:28  2790573696   pt                None  \n",
      "2   2015-02-01 00:07:56  2169009544   es                None  \n",
      "3   2015-02-01 00:08:48  2649502406   pt                None  \n",
      "4   2015-02-01 00:12:14  2769334138   es                None  \n",
      "5   2015-02-01 00:13:03   498201900   pt                None  \n",
      "6   2015-02-01 00:15:04    20916633   pt                None  \n",
      "7   2015-02-01 00:15:04    34986682   pt                None  \n",
      "8   2015-02-01 00:19:23    50374800   pt                None  \n",
      "9   2015-02-01 00:29:25   614468732   pt                None  \n",
      "10  2015-02-01 00:30:10   321070043   pt                None  \n",
      "11  2015-02-01 00:33:09    79073968   pt                None  \n",
      "12  2015-02-01 00:35:33   192037162   pt                None  \n",
      "14  2015-02-01 00:44:13   341329840   pt                None  \n",
      "16  2015-02-01 01:10:04   356916034   pt                None  \n",
      "17  2015-02-01 01:11:52  2800332826   pt                None  \n",
      "18  2015-02-01 01:19:12   744230935   pt                None  \n",
      "23  2015-02-01 01:56:38   930330596   pt                None  \n",
      "24  2015-02-01 02:03:59  1235082847   pt                None  \n",
      "25  2015-02-01 02:07:17    38970838   pt            16632084  \n",
      "26  2015-02-01 02:08:36   122163624   pt                None  \n",
      "27  2015-02-01 02:11:11   100101991   pt            50062276  \n",
      "28  2015-02-01 02:13:46   100101991   pt            50062276  \n",
      "29  2015-02-01 02:15:12  2595454507   pt                None  \n",
      "30  2015-02-01 02:23:13   109101279   pt           312068863  \n",
      "31  2015-02-01 02:32:05    47383723   pt                None  \n",
      "32  2015-02-01 02:32:09    47383723   pt                None  \n",
      "33  2015-02-01 03:04:07   527945055   pt                None  \n",
      "34  2015-02-01 03:08:52    11436952   pt                None  \n",
      "35  2015-02-01 03:12:15  1486067329   es                None  \n",
      "36  2015-02-01 03:12:23   438568354   pt           104002307  \n",
      "37  2015-02-01 03:16:05   887707008   pt                None  \n",
      "38  2015-02-01 03:16:20   566581953   pt                None  \n",
      "39  2015-02-01 03:16:37   887707008   pt                None  \n",
      "40  2015-02-01 03:17:12   533292091   pt            19964288  \n",
      "41  2015-02-01 03:18:11    17675080   pt            55118656  \n",
      "42  2015-02-01 03:18:51  1134966854   pt                None  \n",
      "43  2015-02-01 03:20:40   377749571   pt                None  \n",
      "44  2015-02-01 03:23:05  2714791298   pt                None  \n",
      "45  2015-02-01 04:02:44   197265897   pt                None  \n",
      "46  2015-02-01 04:37:42   400701289   pt                None  \n",
      "47  2015-02-01 05:03:10   336629154   pt                None  \n",
      "48  2015-02-01 05:35:03   113765032   pt                None  \n",
      "49  2015-02-01 05:36:05   393144290   pt                None  \n",
      "50  2015-02-01 05:37:10   174905247   pt                None  \n",
      "51  2015-02-01 05:37:38   253316454   pt                None  \n",
      "52  2015-02-01 05:39:24  1126498818   pt                None  \n",
      "53  2015-02-01 05:44:11    59321805   pt                None  \n",
      "54  2015-02-01 05:45:11   173944935   pt                None  \n",
      "55  2015-02-01 06:46:29    33411162   pt                None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\3411407438.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(mention_pattern, '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match mentions (usernames)\n",
    "mention_pattern = r'@\\w+'\n",
    "\n",
    "# Remove mentions from the 'text' column using regular expressions\n",
    "df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(mention_pattern, '', regex=True)\n",
    "\n",
    "print(df_filtered.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbfd01",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5f9a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\3747244384.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['tokens'] = df_filtered['text'].apply(word_tokenize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0   : A União Europeia tem virado as costas aos r...   \n",
      "1   : A União Europeia tem virado as costas aos r...   \n",
      "2                         Va estar refugiado seguro.   \n",
      "3   : A União Europeia tem virado as costas aos r...   \n",
      "4                                          Refugiado   \n",
      "5   : A União Europeia tem virado as costas aos r...   \n",
      "6    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   : 'A escalada do sofrimento dos refugiados', ...   \n",
      "9                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "\n",
      "                                              tokens  \n",
      "0  [:, A, União, Europeia, tem, virado, as, costa...  \n",
      "1  [:, A, União, Europeia, tem, virado, as, costa...  \n",
      "2                  [Va, estar, refugiado, seguro, .]  \n",
      "3  [:, A, União, Europeia, tem, virado, as, costa...  \n",
      "4                                        [Refugiado]  \n",
      "5  [:, A, União, Europeia, tem, virado, as, costa...  \n",
      "6  [[, Vídeo, ], IMIGRAÇÃO, -, Canal, -, Porta, d...  \n",
      "7  [[, Vídeo, ], IMIGRAÇÃO, -, Canal, -, Porta, d...  \n",
      "8  [:, ', A, escalada, do, sofrimento, dos, refug...  \n",
      "9  [E, EU, FUI, PEGA, NA, IMIGRAÇÃO, !, !, !, !, ...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text in the 'text' column using NLTK\n",
    "df_filtered['tokens'] = df_filtered['text'].apply(word_tokenize)\n",
    "\n",
    "# Display the first 10 rows with tokenized text\n",
    "print(df_filtered[['text', 'tokens']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ff66e",
   "metadata": {},
   "source": [
    "Cleaning 5 - Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e742d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\2906139803.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['filtered_tokens'] = df_filtered['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in portuguese_stopwords])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0   : A União Europeia tem virado as costas aos r...   \n",
      "1   : A União Europeia tem virado as costas aos r...   \n",
      "2                         Va estar refugiado seguro.   \n",
      "3   : A União Europeia tem virado as costas aos r...   \n",
      "4                                          Refugiado   \n",
      "5   : A União Europeia tem virado as costas aos r...   \n",
      "6    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   : 'A escalada do sofrimento dos refugiados', ...   \n",
      "9                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0  [:, União, Europeia, virado, costas, refugiado...  \n",
      "1  [:, União, Europeia, virado, costas, refugiado...  \n",
      "2                         [Va, refugiado, seguro, .]  \n",
      "3  [:, União, Europeia, virado, costas, refugiado...  \n",
      "4                                        [Refugiado]  \n",
      "5  [:, União, Europeia, virado, costas, refugiado...  \n",
      "6  [[, Vídeo, ], IMIGRAÇÃO, -, Canal, -, Porta, F...  \n",
      "7  [[, Vídeo, ], IMIGRAÇÃO, -, Canal, -, Porta, F...  \n",
      "8  [:, ', escalada, sofrimento, refugiados, ', ,,...  \n",
      "9             [PEGA, IMIGRAÇÃO, !, !, !, !, !, !, !]  \n"
     ]
    }
   ],
   "source": [
    "# Additional list of stopwords\n",
    "additional_stopwords = ['a', 'à', 'agora', 'ainda', 'além', 'algo', 'algumas', 'alguns', 'ali', 'ano', 'anos', 'antes', 'ao', 'aos', 'apenas',\n",
    "    'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás',\n",
    "    'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento',\n",
    "    'certamente', 'certeza', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'corrente',\n",
    "    'cuja', 'cujas', 'cujo', 'cujos', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'daqui', 'daquilo',\n",
    "    'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'dêm', 'demais', 'dentro', 'depois', 'desde', 'dessa',\n",
    "    'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'deverá', 'dez', 'dezanove', 'dezasseis',\n",
    "    'dezassete', 'dezoito', 'dia', 'diante', 'diz', 'dizem', 'do', 'dona', 'donas', 'dono', 'donos', 'dos', 'doze', 'duas',\n",
    "    'dúvida', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'entre', 'então', 'entanto', 'era', 'eram', 'éramos',\n",
    "    'é', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'está', 'estão', 'estar', 'estará', 'estas', 'estás', 'estava',\n",
    "    'estavam', 'este', 'estes', 'esteve', 'estive', 'estivemos', 'estiveram', 'estiveste', 'estivestes', 'estou', 'eu',\n",
    "    'exemplo', 'falta', 'fará', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazer', 'fazes', 'fez', 'fim', 'final', 'foi',\n",
    "    'fomos', 'for', 'fora', 'foram', 'formos', 'fosse', 'fossem', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes',\n",
    "    'grupo', 'hoje', 'hora', 'horas', 'ir', 'irá', 'isso', 'isto', 'já', 'lá', 'lado', 'ligado', 'local', 'logo', 'longe',\n",
    "    'lugar', 'maior', 'maioria', 'maiorias', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês',\n",
    "    'meses', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muito', 'muitos', 'na', 'nada', 'naquela', 'naquelas',\n",
    "    'naquele', 'naqueles', 'nas', 'nem', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',\n",
    "    'ninguém', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num',\n",
    "    'numa', 'número', 'nunca', 'nuns', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem',\n",
    "    'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas',\n",
    "    'pelo', 'pelos', 'perto', 'pode', 'pôde', 'podem', 'poderá', 'podia', 'pois', 'ponto', 'pontos', 'por', 'porque', 'porquê',\n",
    "    'pouca', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios',\n",
    "    'quáis', 'qual', 'qualquer', 'quando', 'quanto', 'quarta', 'quarto', 'quatro', 'que', 'quem', 'quer', 'quê', 'quinta',\n",
    "    'quinto', 'quinze', 'relação', 'sabe', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sempre', 'sendo',\n",
    "    'ser', 'será', 'seu', 'seus', 'sexta', 'sexto', 'sim', 'sistema', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas',\n",
    "    'tal', 'talvez', 'também', 'tanta', 'tantas', 'tanto', 'tantos', 'te', 'tem', 'têm', 'temos', 'tendes', 'tenho', 'tens',\n",
    "    'ter', 'terá', 'terão', 'terceira', 'terceiro', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tive', 'tivemos',\n",
    "    'tiveram', 'tiveste', 'tivestes', 'toda', 'todas', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas',\n",
    "    'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'usa', 'usar', 'vai', 'vais', 'valor',\n",
    "    'veja', 'vem', 'vens', 'ver', 'verdade', 'verdadeiro', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'você', 'vocês',\n",
    "    'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero']\n",
    "\n",
    "# Combine both lists of stopwords\n",
    "portuguese_stopwords = stop + additional_stopwords\n",
    "\n",
    "\n",
    "df_filtered['filtered_tokens'] = df_filtered['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in portuguese_stopwords])\n",
    "\n",
    "# Display the first 10 rows with tokenized and filtered text\n",
    "print(df_filtered[['text', 'filtered_tokens']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aeb080",
   "metadata": {},
   "source": [
    "Cleaaning 6 - removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2db06d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anape\\AppData\\Local\\Temp\\ipykernel_21348\\3103169478.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['cleaned_tokens'] = df_filtered['filtered_tokens'].apply(lambda tokens: [re.sub(punctuation_pattern, '', word) for word in tokens])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0   : A União Europeia tem virado as costas aos r...   \n",
      "1   : A União Europeia tem virado as costas aos r...   \n",
      "2                         Va estar refugiado seguro.   \n",
      "3   : A União Europeia tem virado as costas aos r...   \n",
      "4                                          Refugiado   \n",
      "5   : A União Europeia tem virado as costas aos r...   \n",
      "6    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "7    [Vídeo] IMIGRAÇÃO - Canal - Porta dos Fundos -    \n",
      "8   : 'A escalada do sofrimento dos refugiados', ...   \n",
      "9                  E EU FUI PEGA NA IMIGRAÇÃO!!!!!!!   \n",
      "\n",
      "                                      cleaned_tokens  \n",
      "0  [, União, Europeia, virado, costas, refugiados, ]  \n",
      "1  [, União, Europeia, virado, costas, refugiados, ]  \n",
      "2                          [Va, refugiado, seguro, ]  \n",
      "3  [, União, Europeia, virado, costas, refugiados, ]  \n",
      "4                                        [Refugiado]  \n",
      "5  [, União, Europeia, virado, costas, refugiados, ]  \n",
      "6  [, Vídeo, , IMIGRAÇÃO, , Canal, , Porta, Fundo...  \n",
      "7  [, Vídeo, , IMIGRAÇÃO, , Canal, , Porta, Fundo...  \n",
      "8  [, , escalada, sofrimento, refugiados, , , art...  \n",
      "9                    [PEGA, IMIGRAÇÃO, , , , , , , ]  \n"
     ]
    }
   ],
   "source": [
    "# Remove the specified punctuation from filtered tokens\n",
    "df_filtered['cleaned_tokens'] = df_filtered['filtered_tokens'].apply(lambda tokens: [re.sub(punctuation_pattern, '', word) for word in tokens])\n",
    "\n",
    "# Display the first 10 rows with cleaned tokens\n",
    "print(df_filtered[['text', 'cleaned_tokens']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177ba6c",
   "metadata": {},
   "source": [
    "Frenquency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word frequencies using FreqDist\n",
    "word_freq = FreqDist(df_filtered['cleaned_tokens'].explode())\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "most_common_words = word_freq.most_common()\n",
    "for word, freq in most_common_words:\n",
    "    print(f'{word}: {freq}')\n",
    "\n",
    "# Print the length of the filtered DataFrame\n",
    "print(\"Filtered DataFrame length:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b2b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
