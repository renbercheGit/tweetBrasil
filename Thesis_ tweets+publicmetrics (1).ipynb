{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82216917",
   "metadata": {},
   "source": [
    "# Top10 most liked and retweeted tweets (year and month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415366ab",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d1f07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clean data\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from collections import Counter,OrderedDict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739945a",
   "metadata": {},
   "source": [
    "Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8286ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170852\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000016383915253765</td>\n",
       "      <td>@CleberEsteves09, bom dia. A Imigrantes, neste...</td>\n",
       "      <td>2018-05-25 14:10:59</td>\n",
       "      <td>138075168</td>\n",
       "      <td>pt</td>\n",
       "      <td>96735968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000016491390099456</td>\n",
       "      <td>Oposição cabo-verdiana entrega projeto para re...</td>\n",
       "      <td>2018-05-25 14:11:25</td>\n",
       "      <td>1690412382</td>\n",
       "      <td>pt</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000016689931653123</td>\n",
       "      <td>\"Cheguei a São Paulo como refugiado. Tinha 16 ...</td>\n",
       "      <td>2018-05-25 14:12:12</td>\n",
       "      <td>2149024418</td>\n",
       "      <td>pt</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000016951853318145</td>\n",
       "      <td>\"Cheguei a São Paulo como refugiado. Tinha 16 ...</td>\n",
       "      <td>2018-05-25 14:13:15</td>\n",
       "      <td>26411144</td>\n",
       "      <td>pt</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000017172918358016</td>\n",
       "      <td>@Grettux_MG @SenhooraDepp @KaySsicaBaby No tt ...</td>\n",
       "      <td>2018-05-25 14:14:08</td>\n",
       "      <td>818123243924160513</td>\n",
       "      <td>pt</td>\n",
       "      <td>2493509342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1000016383915253765  @CleberEsteves09, bom dia. A Imigrantes, neste...   \n",
       "1  1000016491390099456  Oposição cabo-verdiana entrega projeto para re...   \n",
       "2  1000016689931653123  \"Cheguei a São Paulo como refugiado. Tinha 16 ...   \n",
       "3  1000016951853318145  \"Cheguei a São Paulo como refugiado. Tinha 16 ...   \n",
       "4  1000017172918358016  @Grettux_MG @SenhooraDepp @KaySsicaBaby No tt ...   \n",
       "\n",
       "            created_at           author_id lang in_reply_to_user_id  \n",
       "0  2018-05-25 14:10:59           138075168   pt            96735968  \n",
       "1  2018-05-25 14:11:25          1690412382   pt                None  \n",
       "2  2018-05-25 14:12:12          2149024418   pt                None  \n",
       "3  2018-05-25 14:13:15            26411144   pt                None  \n",
       "4  2018-05-25 14:14:08  818123243924160513   pt          2493509342  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv('tweet_all2.csv',  sep=';;', on_bad_lines='skip',lineterminator='\\r', engine ='python')\n",
    "\n",
    "print(len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000bb1e2",
   "metadata": {},
   "source": [
    "'lang' filter (still not sure if to use it or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5d7e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750071\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where the language is 'pt'\n",
    "df_pt = df[df['lang'] == 'pt']\n",
    "\n",
    "print(len(df_pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfabad2",
   "metadata": {},
   "source": [
    "Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24a227f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame length: 666895\n"
     ]
    }
   ],
   "source": [
    "# Specify the list of specific words you want to filter out\n",
    "specific_words = ['trânsito', 'rodovia', 'tráfego', 'ecovias','TIM', 'tim', 'Kaysar' , 'novela', 'Bandeirantes', 'operacaobetalab', 'bbb', 'transito', 'trânsito', 'orcars', 'oscar', 'grammy', 'grammys', 'kaysar', 'timbetalab', ' masterchefbr', 'enem', 'futebol', ' bundesliga', 'corinthians']\n",
    "\n",
    "# Fill NaN values in the 'text' column with an empty string\n",
    "df_pt['text'].fillna('', inplace=True)\n",
    "\n",
    "# Create a boolean mask for rows containing specific words in the 'text' column\n",
    "mask = df_pt['text'].str.contains('|'.join(specific_words), case=False)\n",
    "\n",
    "# Apply the mask to filter out rows with specific words\n",
    "df_filtered = df_pt[~mask]\n",
    "\n",
    "# Define a regular expression pattern to match URLs\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# Remove URLs from the 'text' column using regular expressions\n",
    "df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(url_pattern, '', regex=True)\n",
    "\n",
    "print(\"Filtered DataFrame length:\", len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0942744",
   "metadata": {},
   "source": [
    "Merging filtered df with public_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "381b0995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame length: 623361\n"
     ]
    }
   ],
   "source": [
    "df1 = df_filtered\n",
    "df2 = pd.read_csv('public_metrics.csv',  sep=';;', on_bad_lines='skip',lineterminator='\\r', engine ='python')\n",
    "\n",
    "# Convert 'id' column in df1 to string\n",
    "df1['id'] = df1['id'].astype(str)\n",
    "\n",
    "# Convert 'tweet' column in df2 to string\n",
    "df2['tweet'] = df2['tweet'].astype(str)\n",
    "\n",
    "# Fill NaN values in the 'tweet' column of df2\n",
    "df2['tweet'].fillna('', inplace=True)\n",
    "\n",
    "# Perform an inner join on 'id' and 'tweet' columns to include only rows with correspondence\n",
    "merged_df = df1.merge(df2, left_on='id', right_on='tweet', how='inner')\n",
    "\n",
    "# Print the length of the new merged dataframe\n",
    "print(\"Filtered DataFrame length:\", len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b36dd",
   "metadata": {},
   "source": [
    "Parsing data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "989825fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at\n",
      "2015    132272\n",
      "2016    148870\n",
      "2017    135837\n",
      "2018     81890\n",
      "2019    124228\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the parse_date function\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT  \n",
    "\n",
    "# Apply the parse_date function to 'created_at' column\n",
    "merged_df['created_at'] = merged_df['created_at'].apply(parse_date)\n",
    "\n",
    "# Drop rows with missing dates\n",
    "merged_df = merged_df.dropna(subset=['created_at'])\n",
    "\n",
    "# Extract the 'date' and 'time' components\n",
    "merged_df['date'] = merged_df['created_at'].dt.date\n",
    "merged_df['time'] = merged_df['created_at'].dt.time\n",
    "\n",
    "# Filter rows for the years 2015 to 2019\n",
    "filtered_df = merged_df[(merged_df['created_at'] >= pd.Timestamp('2015-01-01')) & (merged_df['created_at'] <= pd.Timestamp('2019-12-31'))]\n",
    "\n",
    "# Group the data by year and count the occurrences\n",
    "year_frequency = filtered_df.groupby(filtered_df['created_at'].dt.year)['date'].count()\n",
    "\n",
    "print(year_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b85634",
   "metadata": {},
   "source": [
    "Parsing data by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ce2b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at  created_at\n",
      "2015        2              8497\n",
      "            3              7752\n",
      "            4             10690\n",
      "            5             13144\n",
      "            6             12736\n",
      "            7             11174\n",
      "            8             14122\n",
      "            9             14618\n",
      "            10            13458\n",
      "            11            13563\n",
      "            12            12518\n",
      "2016        1             12814\n",
      "            2             12041\n",
      "            3             12777\n",
      "            4             12486\n",
      "            5             12285\n",
      "            6             12652\n",
      "            7             12433\n",
      "            8             12094\n",
      "            9             12817\n",
      "            10            12811\n",
      "            11            11900\n",
      "            12            11760\n",
      "2017        1             11747\n",
      "            2             12189\n",
      "            3             12206\n",
      "            4             11343\n",
      "            5             12777\n",
      "            6             11887\n",
      "            7             10393\n",
      "            8             11673\n",
      "            9             10063\n",
      "            10            10599\n",
      "            11            10716\n",
      "            12            10244\n",
      "2018        1              9779\n",
      "            2              9479\n",
      "            3              8992\n",
      "            4              8770\n",
      "            5             10137\n",
      "            6             11195\n",
      "            7              3200\n",
      "            11             8461\n",
      "            12            11877\n",
      "2019        1             10214\n",
      "            2              9945\n",
      "            3              9869\n",
      "            4             10112\n",
      "            5             10652\n",
      "            6             10664\n",
      "            7             11493\n",
      "            8             11807\n",
      "            9              9754\n",
      "            10            10235\n",
      "            11             8786\n",
      "            12            10697\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter rows per year\n",
    "filtered_df = merged_df[(merged_df['created_at'] >= pd.Timestamp('2015-01-01')) & (merged_df['created_at'] <= pd.Timestamp('2019-12-31'))]\n",
    "\n",
    "# Group the data by year and month and count the occurrences\n",
    "month_frequency = filtered_df.groupby([filtered_df['created_at'].dt.year, filtered_df['created_at'].dt.month])['date'].count()\n",
    "\n",
    "print(month_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffc408",
   "metadata": {},
   "source": [
    "# LIKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d82a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 186670\n",
      "Likes: 167\n",
      "Text: Jolie: 'É repugnante ver refugiados que se afogam no limiar do continente mais rico do mundo'. http://t.co/PKQ8qWE4t7 http://t.co/FeAifm79Ti\n",
      "==============================\n",
      "Row 181338\n",
      "Likes: 87\n",
      "Text: Imigrantes venezuelanos fazem panelaço perto de hotel onde Maduro está. #CumbrePanama http://t.co/Mng9kcJeQB http://t.co/lJ8AkYd4A2\n",
      "==============================\n",
      "Row 188384\n",
      "Likes: 74\n",
      "Text: Obama premia professora de alunos refugiados e pede melhores salários aos mestres http://t.co/AfWR5LI4Qw #G1 http://t.co/xAo662wmP9\n",
      "==============================\n",
      "Row 184366\n",
      "Likes: 72\n",
      "Text: Filme conta como é ser dançarino em um país onde dançar é pecado. http://t.co/XYl35Jp3Xt http://t.co/EcNc4I2gDN\n",
      "==============================\n",
      "Row 179589\n",
      "Likes: 67\n",
      "Text: 2 mil pessoas são retiradas de campo de refugiados na Síria com avanço do Estado Islâmico http://t.co/2w1ji8BEG4 http://t.co/64539bJTYr\n",
      "==============================\n",
      "Row 184298\n",
      "Likes: 66\n",
      "Text: O #Fantastico foi até a ilha de Lampedusa, na Itália, para mostrar de perto como é a chegada dos refugiados\n",
      "==============================\n",
      "Row 188056\n",
      "Likes: 59\n",
      "Text: Nepal: Índia dará visto gratuito para brasileiros refugiados http://t.co/Mdc4uS0xch http://t.co/Y992Lpr3uW\n",
      "==============================\n",
      "Row 178184\n",
      "Likes: 58\n",
      "Text: Estado Islâmico invade campo de refugiados na Síria. http://t.co/K7iFUq7ncN http://t.co/bIOVAFBYQ1\n",
      "==============================\n",
      "Row 181700\n",
      "Likes: 49\n",
      "Text: Chegaram os jovens refugiados sírios que a mocidade da Lagoinha está cuidando! Nosso coração está muito feliz por... http://t.co/FsE8cI6i35\n",
      "==============================\n",
      "Row 179921\n",
      "Likes: 47\n",
      "Text: Cada momento foi tão especial! Nessas fotos vemos os jovens #refugiados da #Síria que acolhemos em… https://t.co/WDenIu9JbV\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Filter rows per year \n",
    "filtered_df = merged_df[(merged_df['created_at'] >= pd.Timestamp('2015-01-01')) & (merged_df['created_at'] <= pd.Timestamp('2015-12-31'))]\n",
    "\n",
    "# Filter rows per month \n",
    "monthyear_like = filtered_df[(filtered_df['created_at'].dt.year == 2015) & (filtered_df['created_at'].dt.month == 4)]\n",
    "\n",
    "# Get the top 10 rows from the column 'text' along with 'like_count'\n",
    "top_10_likes = monthyear_like.nlargest(10, 'like_count')[['like_count', 'text']]\n",
    "\n",
    "# Print the top 10 rows with 'like_count' and text\n",
    "for index, row in top_10_likes.iterrows():\n",
    "    print(\"Row\", index)\n",
    "    print(\"Likes:\", row['like_count'])\n",
    "    print(\"Text:\", row['text'])\n",
    "    print(\"=\" * 30)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8876c2",
   "metadata": {},
   "source": [
    "# RETWEETS (weird result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8f1a8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 173585\n",
      "Retweets: 163\n",
      "Text: RT @AlfioKrancic: Isis migrante... http://t.co/1kQAXBGrFY\n",
      "==============================\n",
      "Row 173724\n",
      "Retweets: 163\n",
      "Text: RT @AlfioKrancic: Isis migrante... http://t.co/1kQAXBGrFY\n",
      "==============================\n",
      "Row 176797\n",
      "Retweets: 163\n",
      "Text: RT @AlfioKrancic: Isis migrante... http://t.co/1kQAXBGrFY\n",
      "==============================\n",
      "Row 176802\n",
      "Retweets: 163\n",
      "Text: RT @AlfioKrancic: Isis migrante... http://t.co/1kQAXBGrFY\n",
      "==============================\n",
      "Row 175174\n",
      "Retweets: 104\n",
      "Text: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles e suas famílias.\n",
      "==============================\n",
      "Row 175175\n",
      "Retweets: 104\n",
      "Text: RT @SenadorCaiado: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles …\n",
      "==============================\n",
      "Row 175176\n",
      "Retweets: 104\n",
      "Text: RT @SenadorCaiado: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles …\n",
      "==============================\n",
      "Row 175177\n",
      "Retweets: 104\n",
      "Text: RT @SenadorCaiado: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles …\n",
      "==============================\n",
      "Row 175178\n",
      "Retweets: 104\n",
      "Text: RT @SenadorCaiado: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles …\n",
      "==============================\n",
      "Row 175179\n",
      "Retweets: 104\n",
      "Text: RT @SenadorCaiado: Além de convocar Ideli, vou orientar cubanos a pedirem no Comitê Nacional para os Refugiados (Conare) refúgio para eles …\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Filter rows per year \n",
    "filtered_df = merged_df[(merged_df['created_at'] >= pd.Timestamp('2015-01-01')) & (merged_df['created_at'] <= pd.Timestamp('2015-12-31'))]\n",
    "\n",
    "# Filter rows per month \n",
    "monthyearfilter_rt = filtered_df[(filtered_df['created_at'].dt.year == 2015) & (filtered_df['created_at'].dt.month == 3)]\n",
    "\n",
    "# Get the top 10 rows from the column 'text' along with 'like_count'\n",
    "top_10_retweets = monthyearfilter_rt.nlargest(10, 'retweet_count')[['retweet_count', 'text']]\n",
    "\n",
    "# Print the top 10 rows with 'like_count' and text\n",
    "for index, row in top_10_retweets.iterrows():\n",
    "    print(\"Row\", index)\n",
    "    print(\"Retweets:\", row['retweet_count'])\n",
    "    print(\"Text:\", row['text'])\n",
    "    print(\"=\" * 30)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f2623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
