{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82216917",
   "metadata": {},
   "source": [
    "# Data Analysis - All dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415366ab",
   "metadata": {},
   "source": [
    "<h2>Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter,OrderedDict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739945a",
   "metadata": {},
   "source": [
    "<h2>Dataframe</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV():\n",
    "    df =  pd.read_csv('tweet_all2.csv',  sep=';;', on_bad_lines='skip',lineterminator='\\r', engine ='python')\n",
    "    text = df['text']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfabad2",
   "metadata": {},
   "source": [
    "<h2>Clear Tweets</h2>\n",
    "<br>\n",
    "Removing rows with specific words (the whole row, because is related to a subject that won't be useful) - here it also eliminates if the word has is a hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53848ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTweetsIfcontainsWord():\n",
    "    # Specify the list of specific words you want to filter out\n",
    "    specific_words = ['trânsito', 'rodovia', 'tráfego', 'ecovias','TIM', 'tim', 'Kaysar' , 'novela', 'Bandeirantes', 'operacaobetalab', 'bbb', 'transito', 'trânsito', 'orcars', 'oscar', 'grammy', 'grammys', 'kaysar', 'timbetalab', ' masterchefbr', 'enem', 'futebol', ' bundesliga', 'corinthians']\n",
    "\n",
    "    # Fill NaN values in the 'text' column with an empty string\n",
    "    df['text'].fillna('', inplace=True)\n",
    "\n",
    "    # Create a boolean mask for rows containing specific words in the 'text' column\n",
    "    mask = df['text'].str.contains('|'.join(specific_words), case=False)\n",
    "\n",
    "    # Apply the mask to filter out rows with specific words\n",
    "    df_filtered = df[~mask]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ebd8e",
   "metadata": {},
   "source": [
    "<h2>Clear Text</h2>\n",
    "<br>\n",
    "Removing specific elements inside the Text field, like URLs, username, TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURL():\n",
    "    # Define a regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Remove URLs from the 'text' column using regular expressions\n",
    "    df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(url_pattern, '', regex=True)\n",
    "    return df_filtered\n",
    "    \n",
    "def removeListWords():\n",
    "    # Define a list of specific words you want to remove\n",
    "    specific_words_to_remove = ['RT', 'q' , 'vc', 'htt...', 'http...']\n",
    "\n",
    "    # Create a regular expression pattern to match the specific words\n",
    "    pattern = r'\\b(?:' + '|'.join(specific_words_to_remove) + r')\\b'\n",
    "\n",
    "    # Remove the specific words from the 'text' column using regular expressions\n",
    "    df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(pattern, '', regex=True)\n",
    "    return df_filtered\n",
    "\n",
    "def removeUsername():\n",
    "    # Define a regular expression pattern to match mentions (usernames)\n",
    "    mention_pattern = r'@\\w+'\n",
    "\n",
    "    # Remove mentions from the 'text' column using regular expressions\n",
    "    df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(mention_pattern, '', regex=True)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbfd01",
   "metadata": {},
   "source": [
    "<h2>Text Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization():\n",
    "    # Tokenize the text in the 'text' column using NLTK\n",
    "    df_filtered['tokens'] = df_filtered['text'].apply(word_tokenize)\n",
    "    return df_filtered\n",
    "    \n",
    "def printNToken(n):\n",
    "    # Display the first n rows with tokenized text\n",
    "    print(df_filtered[['text', 'tokens']].head(n))\n",
    "    \n",
    "def removeStopwords():\n",
    "    # Additional list of stopwords\n",
    "    additional_stopwords = ['a', 'à', 'agora', 'ainda', 'além', 'algo', 'algumas', 'alguns', 'ali', 'ano', 'anos', 'antes', 'ao', 'aos', 'apenas',\n",
    "    'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás',\n",
    "    'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento',\n",
    "    'certamente', 'certeza', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'corrente',\n",
    "    'cuja', 'cujas', 'cujo', 'cujos', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'daqui', 'daquilo',\n",
    "    'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'dêm', 'demais', 'dentro', 'depois', 'desde', 'dessa',\n",
    "    'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'deverá', 'dez', 'dezanove', 'dezasseis',\n",
    "    'dezassete', 'dezoito', 'dia', 'diante', 'diz', 'dizem', 'do', 'dona', 'donas', 'dono', 'donos', 'dos', 'doze', 'duas',\n",
    "    'dúvida', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'entre', 'então', 'entanto', 'era', 'eram', 'éramos',\n",
    "    'é', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'está', 'estão', 'estar', 'estará', 'estas', 'estás', 'estava',\n",
    "    'estavam', 'este', 'estes', 'esteve', 'estive', 'estivemos', 'estiveram', 'estiveste', 'estivestes', 'estou', 'eu',\n",
    "    'exemplo', 'falta', 'fará', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazer', 'fazes', 'fez', 'fim', 'final', 'foi',\n",
    "    'fomos', 'for', 'fora', 'foram', 'formos', 'fosse', 'fossem', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes',\n",
    "    'grupo', 'hoje', 'hora', 'horas', 'ir', 'irá', 'isso', 'isto', 'já', 'lá', 'lado', 'ligado', 'local', 'logo', 'longe',\n",
    "    'lugar', 'maior', 'maioria', 'maiorias', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês',\n",
    "    'meses', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muito', 'muitos', 'na', 'nada', 'naquela', 'naquelas',\n",
    "    'naquele', 'naqueles', 'nas', 'nem', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',\n",
    "    'ninguém', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num',\n",
    "    'numa', 'número', 'nunca', 'nuns', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem',\n",
    "    'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas',\n",
    "    'pelo', 'pelos', 'perto', 'pode', 'pôde', 'podem', 'poderá', 'podia', 'pois', 'ponto', 'pontos', 'por', 'porque', 'porquê',\n",
    "    'pouca', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios',\n",
    "    'quáis', 'qual', 'qualquer', 'quando', 'quanto', 'quarta', 'quarto', 'quatro', 'que', 'quem', 'quer', 'quê', 'quinta',\n",
    "    'quinto', 'quinze', 'relação', 'sabe', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sempre', 'sendo',\n",
    "    'ser', 'será', 'seu', 'seus', 'sexta', 'sexto', 'sim', 'sistema', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas',\n",
    "    'tal', 'talvez', 'também', 'tanta', 'tantas', 'tanto', 'tantos', 'te', 'tem', 'têm', 'temos', 'tendes', 'tenho', 'tens',\n",
    "    'ter', 'terá', 'terão', 'terceira', 'terceiro', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tive', 'tivemos',\n",
    "    'tiveram', 'tiveste', 'tivestes', 'toda', 'todas', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas',\n",
    "    'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'usa', 'usar', 'vai', 'vais', 'valor',\n",
    "    'veja', 'vem', 'vens', 'ver', 'verdade', 'verdadeiro', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'você', 'vocês',\n",
    "    'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero','.', '!' , '?', '“', '”','`','``',':','\\\\','\\'\\'',',','#','-','(',')','[',']',';','|','/']\n",
    "    \n",
    "    # Combine both lists of stopwords\n",
    "    portuguese_stopwords = stop + additional_stopwords\n",
    "    df_filtered['filtered_tokens'] = df_filtered['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in portuguese_stopwords])\n",
    "    return df_filtered\n",
    "\n",
    "def printNRowsAfterStopword(n):\n",
    "    # Display the first n rows with tokenized and filtered text\n",
    "    print(df_filtered[['text', 'filtered_tokens']].head(n))    \n",
    "    \n",
    "def removePunct():    \n",
    "        # Define a list of specific words you want to remove\n",
    "    specific_words_to_remove = ['.', '!' , '?', '“', '”','`']\n",
    "\n",
    "    # Create a regular expression pattern to match the specific words\n",
    "    pattern = r'\\b(?:' + '|'.join(specific_words_to_remove) + r')\\b'\n",
    "\n",
    "    # Remove the specific words from the 'text' column using regular expressions\n",
    "    df_filtered.loc[:, 'text'] = df_filtered['text'].str.replace(pattern, '', regex=True)\n",
    "    \n",
    "    return df_filtered\n",
    "    #df_filtered['cleaned_tokens'] = df_filtered['filtered_tokens'].apply(lambda tokens: [re.sub(punctuation_pattern, '', word) for word in tokens]\n",
    "    \n",
    "def printNRowsAfterPunct(n):\n",
    "    # Display the first 10 rows with cleaned tokens\n",
    "    print(df_filtered[['text', 'cleaned_tokens']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d087d",
   "metadata": {},
   "source": [
    "<h2>Text Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFrequency():\n",
    "    # Compute word frequencies using FreqDist\n",
    "    word_freq = FreqDist(df_filtered['filtered_tokens'].explode())\n",
    "\n",
    "    #Print the most common words and their frequencies\n",
    "    most_common_words = word_freq.most_common()\n",
    "    for word, freq in most_common_words:\n",
    "        print(f'{word}: {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc798d",
   "metadata": {},
   "source": [
    "<h2>Output information</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNumberOfTweets(df):\n",
    "    print(\"Total amount of tweets\",len(df))\n",
    "\n",
    "def printListAttributes():\n",
    "    print(list(\"Total amount of tweets after filter\",df.columns.values))\n",
    "    \n",
    "def printHead(df,n):\n",
    "    print(df.head(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef412af",
   "metadata": {},
   "source": [
    "<h2>Main</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f322041",
   "metadata": {},
   "source": [
    "1) Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ebdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readCSV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55464bc2",
   "metadata": {},
   "source": [
    "2) Delete <i>Tweets</i> based on specific list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = deleteTweetsIfcontainsWord()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f0b9c",
   "metadata": {},
   "source": [
    "3) Clean Text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a227f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = removeURL()\n",
    "df_filtered = removeListWords()\n",
    "df_filtered = removeUsername()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af81472",
   "metadata": {},
   "outputs": [],
   "source": [
    "printNumberOfTweets(df)\n",
    "printNumberOfTweets(df_filtered)\n",
    "#printHead(df_filtered,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ff66e",
   "metadata": {},
   "source": [
    "4) Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = tokenization()\n",
    "#printNToken(10)\n",
    "\n",
    "df_filtered = removeStopwords()\n",
    "#printNRowsAfterStopword(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177ba6c",
   "metadata": {},
   "source": [
    "5) Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFrequency()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
